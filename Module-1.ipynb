{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cccb2c6-9105-4ff1-b131-d77e7f93c26b",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484c6a3-e2f9-4a10-9886-02bb49cf6403",
   "metadata": {},
   "source": [
    "Itâ€™s a field of Artificial Intelligence (AI) that focuses on how computers can understand, process, and generate human language (text or speech)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59b586-0359-4043-814a-221aec5db4db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lowercasing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da1d64d-0152-4b13-b953-3de628e2fa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am suraj and i live in india\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am Suraj and I live in India\"\n",
    "lowercase_sentence = sentence.lower()\n",
    "print(lowercase_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b8c04d-cca0-40f9-978f-ef12cef87bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my name is suraj', 'i study in jss stu', 'formerly known as sjce']\n"
     ]
    }
   ],
   "source": [
    "sentence_list = [\"My name is Suraj\",\"I study in JSS STU\",\"formerly known as SJCE\"]\n",
    "lowercase_sentence_list = [x.lower() for x in sentence_list]\n",
    "print(lowercase_sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79bd37-65d2-4e16-9fcd-757829842b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871e8752-f2e9-499a-8db8-5600d7509ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SURAJ S\n",
      "[nltk_data]     G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords =  stopwords.words('english')\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13d8b3c-1a50-4639-8904-0caf9d33334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The students waiting bus stop raining, want walk way school wet morning.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The students were waiting at the bus stop because it was raining, and they did not want to walk all the way to the school on a wet morning.\"\n",
    "sentence_no_stopwords = ' '.join([word for word in sentence.split() if word not in en_stopwords])\n",
    "print(sentence_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582908a6-6056-4ac2-8292-d4c18f21017f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42278a8-072a-491d-ba60-722da622789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac395539-44ce-4c50-9c2c-a587386d9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\desktop\\notes\n"
     ]
    }
   ],
   "source": [
    "my_folder = r\"C:\\desktop\\notes\" #raw \n",
    "print(my_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2fd041-2715-45c4-82d2-c56eaad91f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(31, 38), match='pattern'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# .search\n",
    "result_search = re.search(\"pattern\",r\"This sentence contain the word pattern\")\n",
    "print(result_search)\n",
    "\n",
    "result_search2 = re.search(\"pattern\",r\"This sentence does not contain\")\n",
    "print(result_search2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6583466-b53f-47ab-996e-33d53321abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suraj lives in mysuru\n"
     ]
    }
   ],
   "source": [
    "# .sub\n",
    "string = \"Surj lives in mysuru\"\n",
    "new_string = re.sub(\"Surj\",\"Suraj\", string)\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2e3b3c-c80b-48f4-8a28-7be42ae5577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah was able to help me find the items i needed quickly', 'great service from sara she found me what i wanted']\n"
     ]
    }
   ],
   "source": [
    "customer_reviews = ['sam was a great help to me in the store', \n",
    "                    'the cashier was very rude to me, I think her name was eleanor', \n",
    "                    'amazing work from sadeen!', \n",
    "                    'sarah was able to help me find the items i needed quickly', \n",
    "                    'lucy is such a great addition to the team', \n",
    "                    'great service from sara she found me what i wanted'\n",
    "                   ]\n",
    "\n",
    "sarahs_reviews = []\n",
    "\n",
    "pattern_to_find = r\"sarah?\"\n",
    "\n",
    "for string in customer_reviews:\n",
    "    if re.search(pattern_to_find, string):\n",
    "        sarahs_reviews.append(string)\n",
    "\n",
    "print(sarahs_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c2812bd-2dca-4586-a967-39f8af55bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing work from sadeen!']\n"
     ]
    }
   ],
   "source": [
    "a_reviews = []\n",
    "\n",
    "pattern_to_find = r\"^a\"\n",
    "\n",
    "for string in customer_reviews:\n",
    "    if re.search(pattern_to_find, string):\n",
    "        a_reviews.append(string)\n",
    "\n",
    "print(a_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aff0435-6166-4241-a725-d73a33dfb2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah was able to help me find the items i needed quickly']\n"
     ]
    }
   ],
   "source": [
    "y_reviews = []\n",
    "\n",
    "pattern_to_find = r\"y$\"\n",
    "\n",
    "for string in customer_reviews:\n",
    "    if re.search(pattern_to_find, string):\n",
    "        y_reviews.append(string)\n",
    "\n",
    "print(y_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5525bc4-a843-4ffc-876f-fd82a29a2802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah was able to help me find the items i needed quickly', 'great service from sara she found me what i wanted']\n"
     ]
    }
   ],
   "source": [
    "need_want_reviews = []\n",
    "\n",
    "pattern_to_find = r\"(need|want)ed\"\n",
    "\n",
    "for string in customer_reviews:\n",
    "    if re.search(pattern_to_find, string):\n",
    "        need_want_reviews.append(string)\n",
    "\n",
    "print(need_want_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5721ed-33d9-4eeb-8fc0-79e3a7f6f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the cashier was very rude to me, I think her name was eleanor', 'amazing work from sadeen!']\n"
     ]
    }
   ],
   "source": [
    "punct_reviews = []\n",
    "\n",
    "pattern_to_find = r\"[^\\w\\s]\" # ^ - not, \\w - word, \\s - space\n",
    "\n",
    "for string in customer_reviews:\n",
    "    if re.search(pattern_to_find, string):\n",
    "        punct_reviews.append(string)\n",
    "\n",
    "print(punct_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07fd3c5a-98ab-4957-b81a-6e35dd9cd729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sam was a great help to me in the store', 'the cashier was very rude to me I think her name was eleanor', 'amazing work from sadeen', 'sarah was able to help me find the items i needed quickly', 'lucy is such a great addition to the team', 'great service from sara she found me what i wanted']\n"
     ]
    }
   ],
   "source": [
    "no_punct_reviews = []\n",
    "\n",
    "pattern_to_find = r\"[^\\w\\s]\" # ^ - not, \\w - word, \\s - space\n",
    "\n",
    "for string in customer_reviews:\n",
    "    no_punct_reviews.append(re.sub(pattern_to_find, \"\", string))\n",
    "\n",
    "print(no_punct_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fbb22-f901-4366-9774-06ae596326ab",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a53ebb-450e-405d-a551-da5c86f3b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\SURAJ S\n",
      "[nltk_data]     G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c19979-6a80-4f95-a2d9-1d52c2e8862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Suraj.', 'I am twenty years old.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"My name is Suraj. I am twenty years old.\"\n",
    "\n",
    "sent_tokenize(sentences) # sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f497b918-7552-4c0b-b136-628c0b7f7a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'dogs', 'name', 'is', 'Sonu']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentence = \"My dog's name is Sonu!\"\n",
    "\n",
    "word_tokenize(re.sub(r\"[^\\w\\s]\",\"\",word_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c47a0c-1809-4726-bf14-2d137e005029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Latest)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
